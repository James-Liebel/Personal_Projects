{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6a7e25b",
   "metadata": {},
   "source": [
    "# Wide and Deep Neural Network for Adult Census Income Prediction\n",
    "This notebook implements a Wide and Deep Neural Network using **Keras (with PyTorch backend)** to predict income classes from the Adult dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7aea8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Set backend to PyTorch before importing Keras\n",
    "os.environ[\"KERAS_BACKEND\"] = \"torch\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras import layers, Model, Input, ops\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "keras.utils.set_random_seed(42)\n",
    "\n",
    "print(f\"Keras version: {keras.__version__}\")\n",
    "print(f\"Backend: {keras.config.backend()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3f311f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "# The dataset does not have headers, so we define them manually\n",
    "column_names = [\n",
    "    \"age\", \"workclass\", \"fnlwgt\", \"education\", \"education_num\",\n",
    "    \"marital_status\", \"occupation\", \"relationship\", \"race\", \"gender\",\n",
    "    \"capital_gain\", \"capital_loss\", \"hours_per_week\", \"native_country\", \"income\"\n",
    "]\n",
    "\n",
    "# Read from the zip file located in datasets/\n",
    "print(\"Loading data...\")\n",
    "try:\n",
    "    df = pd.read_csv('datasets/adult_test.csv.zip', names=column_names, skipinitialspace=True)\n",
    "except Exception as e:\n",
    "    print(f\"Error loading zip directly: {e}\")\n",
    "    # Fallback if needed\n",
    "\n",
    "# Inspect the stored dataframe\n",
    "print(f\"Dataset Shape: {df.shape}\")\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4733ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Cleaning\n",
    "\n",
    "# The dataset might contain rows that are not actual data (like the 1x3 cross validator comment)\n",
    "df = df.replace('?', np.nan)\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# The target 'income' column in the test set usually ends with a dot (e.g., '<=50K.', '>50K.')\n",
    "# Let's clean it up\n",
    "df['income'] = df['income'].astype(str).str.rstrip('.')\n",
    "\n",
    "# Verify target values\n",
    "print(\"Target value counts:\")\n",
    "print(df['income'].value_counts())\n",
    "\n",
    "# Map to binary\n",
    "df['income'] = df['income'].apply(lambda x: 1 if '>50K' in x else 0)\n",
    "\n",
    "print(f\"Cleaned Dataset Shape: {df.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65ef804",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering\n",
    "\n",
    "# Define feature groups\n",
    "CATEGORICAL_COLS = [\"workclass\", \"education\", \"marital_status\", \"occupation\", \"relationship\", \"race\", \"gender\", \"native_country\"]\n",
    "CONTINUOUS_COLS = [\"age\", \"fnlwgt\", \"education_num\", \"capital_gain\", \"capital_loss\", \"hours_per_week\"]\n",
    "\n",
    "# Prepare Input Data\n",
    "\n",
    "# 1. Continuous Features: Standard Scaling\n",
    "scaler = StandardScaler()\n",
    "X_continuous = scaler.fit_transform(df[CONTINUOUS_COLS])\n",
    "\n",
    "# 2. Categorical Features for Deep Part: Label Encoding\n",
    "X_categorical_indices = []\n",
    "vocab_sizes = {}\n",
    "\n",
    "for col in CATEGORICAL_COLS:\n",
    "    le = LabelEncoder()\n",
    "    df[col] = df[col].astype(str)\n",
    "    col_indices = le.fit_transform(df[col])\n",
    "    X_categorical_indices.append(col_indices)\n",
    "    vocab_sizes[col] = len(le.classes_)\n",
    "\n",
    "X_categorical_indices = np.stack(X_categorical_indices, axis=1)\n",
    "\n",
    "# 3. Categorical Features for Wide Part: One-Hot Encoding\n",
    "df_onehot = pd.get_dummies(df[CATEGORICAL_COLS])\n",
    "X_wide = df_onehot.values.astype('float32') # Ensure float32 for Keras\n",
    "\n",
    "# Split Data\n",
    "y = df['income'].values.astype('float32')\n",
    "\n",
    "indices = np.arange(len(y))\n",
    "train_idx, test_idx = train_test_split(indices, test_size=0.2, random_state=42)\n",
    "\n",
    "X_wide_train, X_wide_test = X_wide[train_idx], X_wide[test_idx]\n",
    "X_deep_cat_train, X_deep_cat_test = X_categorical_indices[train_idx], X_categorical_indices[test_idx]\n",
    "X_deep_cont_train, X_deep_cont_test = X_continuous[train_idx], X_continuous[test_idx]\n",
    "y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "print(f\"Train samples: {len(y_train)}\")\n",
    "print(f\"Test samples: {len(y_test)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2bf0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Wide and Deep Model\n",
    "\n",
    "# Input Layers\n",
    "input_wide = Input(shape=(X_wide.shape[1],), name='wide_input')\n",
    "input_deep_cont = Input(shape=(X_continuous.shape[1],), name='deep_continuous_input')\n",
    "input_deep_cats = [Input(shape=(1,), name=f'deep_cat_{col}') for col in CATEGORICAL_COLS]\n",
    "\n",
    "# Deep Part: Embeddings + Continuous\n",
    "embeddings = []\n",
    "# Ensure continuous input is float32\n",
    "deep_cont_cast = layers.Cast('float32')(input_deep_cont)\n",
    "\n",
    "for i, col in enumerate(CATEGORICAL_COLS):\n",
    "    voc_size = vocab_sizes[col]\n",
    "    emb_dim = min(50, (voc_size + 1) // 2)\n",
    "    # Embedding expects integer indices\n",
    "    emb = layers.Embedding(input_dim=voc_size, output_dim=emb_dim)(input_deep_cats[i])\n",
    "    emb = layers.Flatten()(emb)\n",
    "    embeddings.append(emb)\n",
    "\n",
    "deep_features = layers.concatenate(embeddings + [deep_cont_cast])\n",
    "deep_hidden = layers.Dense(128, activation='relu')(deep_features)\n",
    "deep_hidden = layers.Dropout(0.3)(deep_hidden)\n",
    "deep_hidden = layers.Dense(64, activation='relu')(deep_hidden)\n",
    "deep_hidden = layers.Dropout(0.3)(deep_hidden)\n",
    "\n",
    "# Wide Part\n",
    "# Combined\n",
    "combined = layers.concatenate([input_wide, deep_hidden])\n",
    "\n",
    "# Final Output\n",
    "output = layers.Dense(1, activation='sigmoid')(combined)\n",
    "\n",
    "# Create Model\n",
    "model_inputs = [input_wide] + [input_deep_cont] + input_deep_cats\n",
    "model = Model(inputs=model_inputs, outputs=output)\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy', 'AUC'])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a47319",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare inputs list for training\n",
    "def prepare_inputs(X_wide_d, X_deep_cont_d, X_deep_cat_d):\n",
    "    cat_inputs = [X_deep_cat_d[:, i] for i in range(X_deep_cat_d.shape[1])]\n",
    "    return [X_wide_d, X_deep_cont_d] + cat_inputs\n",
    "\n",
    "train_inputs = prepare_inputs(X_wide_train, X_deep_cont_train, X_deep_cat_train)\n",
    "test_inputs = prepare_inputs(X_wide_test, X_deep_cont_test, X_deep_cat_test)\n",
    "\n",
    "# Train\n",
    "# Keras 3 with Torch works similarly to standard Keras\n",
    "history = model.fit(\n",
    "    train_inputs, y_train,\n",
    "    epochs=15,\n",
    "    batch_size=64,\n",
    "    validation_data=(test_inputs, y_test)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b11a49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "metrics = model.evaluate(test_inputs, y_test)\n",
    "# Keras 3 model.evaluate returns valid list or dict depending on usage, usually list of scalars if no return_dict=True\n",
    "# But let's check structure\n",
    "print(f\"Metrics: {metrics}\")\n",
    "\n",
    "# Plot History\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Val Loss')\n",
    "plt.legend()\n",
    "plt.title('Loss')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['accuracy'], label='Train Acc')\n",
    "plt.plot(history.history['val_accuracy'], label='Val Acc')\n",
    "plt.legend()\n",
    "plt.title('Accuracy')\n",
    "plt.show()\n",
    "\n",
    "# Predictions for Confusion Matrix\n",
    "y_pred_prob = model.predict(test_inputs)\n",
    "y_pred = (y_pred_prob > 0.5).astype(int)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.show()\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
