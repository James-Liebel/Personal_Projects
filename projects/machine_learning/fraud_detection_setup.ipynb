{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "19ba0bf2",
            "metadata": {},
            "source": [
                "\n",
                "# Credit Card Fraud Detection System\n",
                "## Professional Implementation\n",
                "\n",
                "### Project Overview\n",
                "This project implements a comprehensive machine learning pipeline for detecting fraudulent credit card transactions. \n",
                "The dataset typically used is the [Kaggle Credit Card Fraud Detection dataset](https://www.kaggle.com/mlg-ulb/creditcardfraud), dealing with highly imbalanced data.\n",
                "\n",
                "### Objectives\n",
                "1.  **Data Analysis**: Understand the distribution of legitimate vs. fraudulent transactions.\n",
                "2.  **Preprocessing**: Handle class imbalance and scale numerical features.\n",
                "3.  **Modeling**: Implement Logistic Regression, Random Forest, and XGBoost classifiers.\n",
                "4.  **Evaluation**: Use professional metrics: Confusion Matrix, Precision-Recall, F1-Score, and ROC-AUC.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "2628a416",
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "# Installation of necessary libraries\n",
                "# !pip install pandas numpy matplotlib seaborn scikit-learn xgboost\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "\n",
                "# Sklearn Metrics & Preprocessing\n",
                "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\n",
                "from sklearn.preprocessing import RobustScaler\n",
                "from sklearn.metrics import (confusion_matrix, classification_report, accuracy_score, \n",
                "                             precision_score, recall_score, f1_score, roc_auc_score, \n",
                "                             roc_curve, precision_recall_curve, auc)\n",
                "from sklearn.linear_model import LogisticRegression\n",
                "from sklearn.ensemble import RandomForestClassifier\n",
                "try:\n",
                "    import xgboost as xgb\n",
                "except ImportError:\n",
                "    print(\"XGBoost not installed. It is recommended for best performance.\")\n",
                "\n",
                "# Visualization Settings\n",
                "%matplotlib inline\n",
                "sns.set(style=\"whitegrid\", palette=\"muted\")\n",
                "plt.rcParams['figure.figsize'] = (12, 6)\n",
                "print(\"Libraries Setup Complete\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "2b2cdae6",
            "metadata": {},
            "source": [
                "\n",
                "### 1. Data Loading and Inspection\n",
                "We expect the dataset `creditcard.csv` to be located in the `datasets` folder.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "e68bcbbc",
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "import os\n",
                "\n",
                "# Define path\n",
                "DATA_PATH = './datasets/creditcard.csv.zip'\n",
                "\n",
                "if not os.path.exists(DATA_PATH):\n",
                "    print(f\"WARNING: Dataset not found at {DATA_PATH}\")\n",
                "    print(\"Please download 'creditcard.csv.zip' from Kaggle and place it in the 'datasets' directory.\")\n",
                "    # For demonstration purposes, we will stop here if data is missing, \n",
                "    # but in a real run, this block would load the data.\n",
                "    data_exists = False\n",
                "else:\n",
                "    df = pd.read_csv(DATA_PATH, compression='zip')\n",
                "    print(\"Dataset loaded successfully.\")\n",
                "    print(f\"Shape: {df.shape}\")\n",
                "    display(df.head())\n",
                "    data_exists = True\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "26fbe0be",
            "metadata": {},
            "source": [
                "\n",
                "### 2. Exploratory Data Analysis (EDA)\n",
                "Understanding the class imbalance is crucial.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "3ccb0ed9",
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "if data_exists:\n",
                "    # Class distribution\n",
                "    count_classes = pd.value_counts(df['Class'], sort=True)\n",
                "    count_classes.plot(kind='bar', rot=0)\n",
                "    plt.title(\"Transaction Class Distribution\")\n",
                "    plt.xticks(range(2), [\"Normal\", \"Fraud\"])\n",
                "    plt.xlabel(\"Class\")\n",
                "    plt.ylabel(\"Frequency\")\n",
                "    plt.show()\n",
                "\n",
                "    fraud = df[df['Class'] == 1]\n",
                "    normal = df[df['Class'] == 0]\n",
                "    print(f\"Fraudulent transactions: {fraud.shape[0]} ({round(fraud.shape[0]/len(df) * 100, 2)}%)\")\n",
                "    print(f\"Normal transactions: {normal.shape[0]}\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "1ecee0c0",
            "metadata": {},
            "source": [
                "\n",
                "### 3. Data Preprocessing\n",
                "- **Scaling**: We use `RobustScaler` as it is less prone to outliers.\n",
                "- **Splitting**: We split into Training and Test sets *before* any oversampling to avoid data leakage.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "f94eaa80",
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "if data_exists:\n",
                "    # Scaling Time and Amount\n",
                "    rob_scaler = RobustScaler()\n",
                "\n",
                "    df['scaled_amount'] = rob_scaler.fit_transform(df['Amount'].values.reshape(-1,1))\n",
                "    df['scaled_time'] = rob_scaler.fit_transform(df['Time'].values.reshape(-1,1))\n",
                "\n",
                "    df.drop(['Time','Amount'], axis=1, inplace=True)\n",
                "    \n",
                "    # Move scaled columns to front (optional, for clarity)\n",
                "    scaled_amount = df['scaled_amount']\n",
                "    scaled_time = df['scaled_time']\n",
                "    df.drop(['scaled_amount', 'scaled_time'], axis=1, inplace=True)\n",
                "    df.insert(0, 'scaled_amount', scaled_amount)\n",
                "    df.insert(1, 'scaled_time', scaled_time)\n",
                "\n",
                "    # Separation of Input and Output\n",
                "    X = df.drop('Class', axis=1)\n",
                "    y = df['Class']\n",
                "\n",
                "    # Stratified Split\n",
                "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
                "    \n",
                "    print(\"Data validation split successful.\")\n",
                "    print(f\"Training set: {X_train.shape[0]}\")\n",
                "    print(f\"Testing set: {X_test.shape[0]}\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "5441f391",
            "metadata": {},
            "source": [
                "\n",
                "### 4. Model Building & Evaluation\n",
                "We will establish a baseline with Logistic Regression and then try more complex models like Random Forest and XGBoost.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "3d0b5eec",
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "def evaluate_model(model, X_test, y_test, model_name=\"Model\"):\n",
                "    y_pred = model.predict(X_test)\n",
                "    y_prob = model.predict_proba(X_test)[:, 1] if hasattr(model, \"predict_proba\") else y_pred\n",
                "    \n",
                "    print(f\"--- {model_name} Evaluation ---\")\n",
                "    \n",
                "    # 1. Confusion Matrix\n",
                "    cm = confusion_matrix(y_test, y_pred)\n",
                "    plt.figure(figsize=(6, 4))\n",
                "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
                "    plt.title(f'Confusion Matrix: {model_name}')\n",
                "    plt.ylabel('True Label')\n",
                "    plt.xlabel('Predicted Label')\n",
                "    plt.show()\n",
                "    \n",
                "    # 2. Classification Report\n",
                "    print(\"Classification Report:\")\n",
                "    print(classification_report(y_test, y_pred))\n",
                "    \n",
                "    # 3. ROC-AUC Score\n",
                "    try:\n",
                "        roc = roc_auc_score(y_test, y_prob)\n",
                "        print(f\"ROC-AUC Score: {roc:.4f}\")\n",
                "    except:\n",
                "        pass\n",
                "        \n",
                "    # 4. Neural Network / Other metrics if needed\n",
                "    print(\"-\" * 30)\n",
                "    return cm\n",
                "\n",
                "if data_exists:\n",
                "    # Logistic Regression\n",
                "    lr = LogisticRegression(solver='liblinear') # robust to small datasets/simple\n",
                "    lr.fit(X_train, y_train)\n",
                "    evaluate_model(lr, X_test, y_test, \"Logistic Regression\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "7a679d6c",
            "metadata": {},
            "source": [
                "\n",
                "### 5. Advanced Modeling: XGBoost\n",
                "XGBoost is often the gold standard for tabular data classification.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "0fd97fd7",
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "if data_exists:\n",
                "    try:\n",
                "        xgb_clf = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
                "        xgb_clf.fit(X_train, y_train)\n",
                "        evaluate_model(xgb_clf, X_test, y_test, \"XGBoost Classifier\")\n",
                "    except Exception as e:\n",
                "        print(f\"XGBoost training failed or skipped: {e}\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "1eb1e236",
            "metadata": {},
            "source": [
                "\n",
                "### 6. Metric Explanation\n",
                "**Why Confusion Matrix?**\n",
                "In fraud detection, accuracy is misleading because 99.8% of transactions are valid. A model that predicts \"Valid\" for everything has 99.8% accuracy but catches 0 fraud.\n",
                "\n",
                "- **True Positives (TP)**: Fraud correctly identified. (High Priority)\n",
                "- **False Negatives (FN)**: Fraud missed. (Costly!)\n",
                "- **False Positives (FP)**: Legitimate transaction flagged as fraud. (Customer friction)\n",
                "\n",
                "We optimize for **Recall** (catching as much fraud as possible) while maintaining reasonable **Precision** (not annoying too many customers).\n"
            ]
        }
    ],
    "metadata": {},
    "nbformat": 4,
    "nbformat_minor": 5
}